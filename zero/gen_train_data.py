# import os
# import pandas as pd
# import random
# import numpy as np
# import threading
# import math
# from tqdm import tqdm
# import shutil
# pd.set_option('future.no_silent_downcasting', True)


# def gen_group_train_data(groups, daily_data):
#     current_dir = os.path.dirname(os.path.abspath(__file__))
#     train_folder = os.path.join(current_dir, 'train')
#     for i in tqdm(range(len(groups))):
#         group = groups[i][1]
#         # å»é™¤æœ€å7ä¸ªå­£åº¦çš„æ•°æ®
#         group = group.iloc[:-7]
#         group = group.fillna(0).reset_index(drop=True)
#         group_data_length = len(group)
#         # ç›®å‰å¿…é¡»ä¸Šå¸‚3ä¸ªå­£åº¦åæ‰å¯ä»¥é¢„æµ‹
#         if group_data_length < 3:
#             continue
#         symbol = groups[i][0]
#         group_daily = daily_data[(daily_data['symbol'] == symbol)].reset_index(drop=True)

#         for j in range(3, group_data_length-1):
#             group['endDate'] = group['endDate'].astype('int32')
#             endDate = group['endDate'].iloc[j]
#             data_basename = symbol + '_' + str(endDate) + '.h5'
#             # data_name = 'train/' + data_basename
#             data_name = os.path.join(train_folder, data_basename)

#             # æˆªå–dailyæ•°æ®
#             fore_daily = group_daily[(group_daily['date'] > int(endDate))]
#             fore_daily = fore_daily.iloc[:127*3]
#             fore_daily = fore_daily.reset_index(drop=True)
#             if len(fore_daily) != 127*3:
#                 continue
#             min_fore = fore_daily['close'].min()
#             median_fore = fore_daily['close'].median()
#             max_fore = fore_daily['close'].max()
#             price_fore = math.log(min_fore) + math.log(median_fore) + math.log(max_fore)

#             past_daily = group_daily[(group_daily['date'] <= int(endDate))]
#             past_daily = past_daily.iloc[-127*3:]
#             past_daily = past_daily.reset_index(drop=True)
#             if len(past_daily) != 127*3:
#                 continue
#             min_past = past_daily['close'].min()
#             median_past = past_daily['close'].median()
#             max_past = past_daily['close'].max()
#             price_past = math.log(min_past) + math.log(median_past) + math.log(max_past)
#             three = price_fore - price_past
#             if three > 127 or three < -127:
#                 continue
#             # ä¸‹é¢è¿›è¡Œå½’ä¸€åŒ–
#             data = group.iloc[j - 2:j + 1].reset_index(drop=True)
#             if len(data) != 3:
#                 continue
#             data.drop(columns=['symbol'], inplace=True)
#             data.drop(columns=['endDate'], inplace=True)
#             data = data.assign(three=three)
#             data = data.fillna(0)
#             data.replace([np.inf, -np.inf], 0, inplace=True)
#             for col in data.select_dtypes(include=['int64']).columns:
#                 data[col] = data[col].astype('float32')
#             for col in data.select_dtypes(include=['float64']).columns:
#                 data[col] = data[col].astype('float32')
#             for col in data.select_dtypes(include=['object']).columns:
#                 data[col] = data[col].astype('float32')
#             data.to_hdf(data_name, key='data', mode='w')


# def gen_exchange_ratio_train_data(exchange):
#     upper_exchange = exchange[0].upper() + exchange[1:]

#     # åŠ è½½æ•°æ®
#     current_dir = os.path.dirname(os.path.abspath(__file__))
#     daily_path = os.path.join(current_dir, '../data/'+ upper_exchange + '/daily_' + exchange + '.csv')
#     daily_data = pd.read_csv(daily_path, encoding="utf-8")
#     ratio_path = os.path.join(current_dir, '../data/'+ upper_exchange + '/ratio_' + exchange + '.csv')
#     ratio_data = pd.read_csv(ratio_path, encoding="utf-8")
#     print(ratio_data)

#     groups = list(ratio_data.groupby('symbol'))
#     random.shuffle(groups)
#     group_count = len(groups)
#     split_count = int(0.333*group_count)
#     groups_0 = groups[:split_count]
#     groups_1 = groups[split_count:2*split_count]
#     groups_2 = groups[2*split_count:]

#     # åˆ›å»ºçº¿ç¨‹å¹¶å¯åŠ¨å®ƒä»¬
#     thread0 = threading.Thread(target=gen_group_train_data, args=(groups_0, daily_data))
#     thread1 = threading.Thread(target=gen_group_train_data, args=(groups_1, daily_data))
#     thread2 = threading.Thread(target=gen_group_train_data, args=(groups_2, daily_data))

#     thread0.start()
#     thread1.start()
#     thread2.start()

#     thread0.join()
#     thread1.join()
#     thread2.join()


# def gen_ratio_train_data():
#     current_dir = os.path.dirname(os.path.abspath(__file__))
#     train_folder = os.path.join(current_dir, 'train')
#     shutil.rmtree(train_folder, ignore_errors=True)
#     os.makedirs(train_folder, exist_ok=True)
#     # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
#     csv_path = os.path.join(current_dir, '..', 'train_exchanges.csv')
#     csv_path = os.path.abspath(csv_path)  # è½¬æˆç»å¯¹è·¯å¾„

#     exchanges_data = pd.read_csv(csv_path, encoding="utf-8")
#     exchange_list = exchanges_data['exchange'].tolist()
#     failed_list = []
#     for exchange in exchange_list:
#         try:
#             print(exchange)
#             gen_exchange_ratio_train_data(exchange)
#         except:
#             failed_list.append(exchange)

#     print(failed_list)


# if __name__ == '__main__':
#     gen_ratio_train_data()


import os
import pandas as pd
import random
import numpy as np
import math
from tqdm import tqdm
import shutil
from multiprocessing import Pool, cpu_count

pd.set_option('future.no_silent_downcasting', True)


def process_group(group_tuple, daily_data, train_folder):
    """
    å¤„ç†å•ä¸ªè‚¡ç¥¨æ•°æ®ç»„å¹¶ç”Ÿæˆè®­ç»ƒæ–‡ä»¶ã€‚
    è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å‡½æ•°ï¼Œé€‚åˆåœ¨å¤šè¿›ç¨‹ä¸­è¿è¡Œã€‚
    """
    symbol = group_tuple[0]
    group = group_tuple[1]

    # 1. æ•°æ®æ¸…æ´—å’Œå‡†å¤‡
    # å»é™¤æœ€å7ä¸ªå­£åº¦çš„æ•°æ®
    group = group.iloc[:-7]
    group = group.fillna(0).reset_index(drop=True)
    group_data_length = len(group)

    if group_data_length < 3:
        return 0 # è¿”å›æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°ï¼Œè¿™é‡Œæ˜¯ 0

    # ä¼˜åŒ–ï¼šæå‰ç­›é€‰ daily_data
    group_daily = daily_data[daily_data['symbol'] == symbol].reset_index(drop=True)
    if group_daily.empty:
        return 0
    
    # æå‰è½¬æ¢ç±»å‹
    group['endDate'] = group['endDate'].astype('int32')

    files_created = 0

    # å¾ªç¯ç”Ÿæˆè®­ç»ƒæ ·æœ¬
    for j in range(2, group_data_length-1):
        endDate = group['endDate'].iloc[j] # è¿™é‡Œçš„ endDate åº”è¯¥æ˜¯å½“å‰æ ·æœ¬çš„æœ€æ–°è´¢åŠ¡æŠ¥å‘ŠæœŸ

        # é¢„æµ‹æœŸ daily æ•°æ®
        fore_daily = group_daily[group_daily['date'] > int(endDate)]
        fore_daily = fore_daily.iloc[:127*3] # æˆªå–å‰ 3 ä¸ªå­£åº¦ï¼ˆçº¦ 127*3 å¤©ï¼‰
        
        if len(fore_daily) != 127*3:
            continue
        # æ£€æŸ¥é¢„æµ‹æœŸä»·æ ¼æ˜¯å¦æœ‰éæ­£æ•°ï¼ˆ0æˆ–è´Ÿæ•°ï¼‰
        if (fore_daily['close'] <= 0).any():
            # å‘ç°éæ­£æ•°ä»·æ ¼ï¼Œè·³è¿‡æ­¤æ ·æœ¬
            continue
        # ä¼˜åŒ–ï¼šä½¿ç”¨ .agg(['min', 'median', 'max']) ç®€åŒ–æ“ä½œ
        fore_stats = fore_daily['close'].agg(['min', 'median', 'max'])
        price_fore = sum(math.log(x) for x in fore_stats)

        # å†å²æœŸ daily æ•°æ® (åœ¨ endDate_split ä¹‹å‰ï¼Œå–æœ€è¿‘ 127*3 å¤©)
        past_daily = group_daily[group_daily['date'] <= int(endDate)]
        past_daily = past_daily.iloc[-127*3:]
        
        if len(past_daily) != 127*3:
            continue
        # æ£€æŸ¥å†å²æœŸä»·æ ¼æ˜¯å¦æœ‰éæ­£æ•°ï¼ˆ0æˆ–è´Ÿæ•°ï¼‰
        if (past_daily['close'] <= 0).any():
            # å‘ç°éæ­£æ•°ä»·æ ¼ï¼Œè·³è¿‡æ­¤æ ·æœ¬
            continue
        # ä¼˜åŒ–ï¼šä½¿ç”¨ .agg(['min', 'median', 'max']) ç®€åŒ–æ“ä½œ
        past_stats = past_daily['close'].agg(['min', 'median', 'max'])
        price_past = sum(math.log(x) for x in past_stats)
        
        three = price_fore - price_past
        
        if abs(three) > 127: # å¼‚å¸¸å€¼è¿‡æ»¤
            continue           
        
        # æˆªå– 3 ä¸ªå­£åº¦çš„è´¢åŠ¡æ•°æ® (j-2, j-1, j)
        data = group.iloc[j - 2:j + 1, 5:12].copy().reset_index(drop=True) # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
        if len(data) != 3:
            continue
        # data.drop(columns=['symbol', 'endDate', 'netAssetValuePerShare', 'dcfPerShare', 'dividendPerShare'], inplace=True)
        data = data.assign(three=three)
        data = data.fillna(0)
        data.replace([np.inf, -np.inf], 0, inplace=True)
        data[(data > 127.0)] = 127.0
        data[(data < -127.0)] = -127.0
        for col in data.columns:
            if data[col].dtype in ['int64', 'float64', 'object']:
                data[col] = data[col].astype('float32')

        # ä¿å­˜ä¸º HDF5
        data_basename = f"{symbol}_{endDate}.h5"
        data_name = os.path.join(train_folder, data_basename)
        
        # ä½¿ç”¨ 'a' æ¨¡å¼å†™å…¥å•ä¸ªæ–‡ä»¶æ›´å®‰å…¨ï¼Œè™½ç„¶ 'w' ä¹Ÿå¯ä»¥
        data.to_hdf(data_name, key='data', mode='w') 
        files_created += 1
        
    return files_created


def gen_exchange_zero_train_data(exchange):
    """
    åŠ è½½æ•°æ®ï¼Œåˆ†å‰²ä»»åŠ¡å¹¶ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ã€‚
    """
    upper_exchange = exchange[0].upper() + exchange[1:]
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # 1. åŠ è½½æ•°æ®
    daily_path = os.path.join(current_dir, f'../data/{upper_exchange}/daily_{exchange}.csv')
    indicator_path = os.path.join(current_dir, f'../data/{upper_exchange}/indicator_{exchange}.csv')
    
    print(f"Loading {exchange} data...")
    try:
        daily_data = pd.read_csv(daily_path, encoding="utf-8")
        indicator_data = pd.read_csv(indicator_path, encoding="utf-8")
    except FileNotFoundError:
        print(f"Error: Data files not found for {exchange}.")
        return 0
        
    # 2. åˆ†ç»„å¹¶æ‰“ä¹±é¡ºåº
    groups = list(indicator_data.groupby('symbol'))
    random.shuffle(groups)

    # 3. è®¾ç½®å¤šè¿›ç¨‹å‚æ•°
    train_folder = os.path.join(current_dir, 'train')
    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨ CPU æ ¸å¿ƒï¼Œæˆ–æ ¹æ®éœ€è¦è®¾ç½®ä¸€ä¸ªå›ºå®šå€¼
    num_processes = cpu_count() 
    print(f"Starting {num_processes} processes for {len(groups)} groups.")
    
    # å‡†å¤‡ Pool.starmap éœ€è¦çš„å‚æ•°åˆ—è¡¨
    # (group_tuple, daily_data, train_folder)
    task_args = [(group_tuple, daily_data, train_folder) for group_tuple in groups]
    
    # 4. è¿è¡Œå¤šè¿›ç¨‹æ± 
    total_files_created = 0
    try:
        # ä½¿ç”¨ Pool.starmap å¹¶è¡Œå¤„ç†æ‰€æœ‰ groups
        with Pool(processes=num_processes) as pool:
            # è¿›ç¨‹æ± ä¼šè¿”å›ä¸€ä¸ªç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœæ˜¯ process_group çš„è¿”å›å€¼ (files_created)
            results = list(tqdm(pool.starmap(process_group, task_args), total=len(groups), desc=f"Processing {exchange} groups"))
        
        total_files_created = sum(results)
        print(f"Finished {exchange}. Total files created: {total_files_created}")
        
    except Exception as e:
        print(f"An error occurred during multiprocessing for {exchange}: {e}")
        return 0

    return total_files_created


def gen_zero_train_data():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    train_folder = os.path.join(current_dir, 'train')
    
    # æ¸…ç†å’Œåˆ›å»ºè®­ç»ƒç›®å½•
    print("Clearing and creating 'train' folder...")
    shutil.rmtree(train_folder, ignore_errors=True)
    os.makedirs(train_folder, exist_ok=True)
    
    # è·å–äº¤æ˜“æ‰€åˆ—è¡¨
    csv_path = os.path.join(current_dir, '..', 'train_exchanges.csv')
    csv_path = os.path.abspath(csv_path)

    try:
        exchanges_data = pd.read_csv(csv_path, encoding="utf-8")
        exchange_list = exchanges_data['exchange'].tolist()
    except FileNotFoundError:
        print(f"Error: train_exchanges.csv not found at {csv_path}")
        return

    failed_list = []
    total_files = 0
    
    for exchange in exchange_list:
        try:
            print(f"\n--- Starting processing for {exchange} ---")
            files_created = gen_exchange_zero_train_data(exchange)
            if files_created == 0:
                 failed_list.append(exchange)
            total_files += files_created
        except Exception as e:
            print(f"FATAL error for exchange {exchange}: {e}")
            failed_list.append(exchange)

    print("\n--- Summary ---")
    print(f"Total HDF5 files created: {total_files}")
    if failed_list:
        print(f"Failed to process exchanges: {failed_list} âš ï¸")
    else:
        print("All exchanges processed successfully! ğŸ‰")

if __name__ == '__main__':
    gen_zero_train_data()