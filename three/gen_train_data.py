import os
import pandas as pd
import random
import numpy as np
import math
import threading
from tqdm import tqdm
import shutil
import datetime
from multiprocessing import Pool, cpu_count

pd.set_option('future.no_silent_downcasting', True)


# #æ±‚å–ä¸Šä¸ªå­£åº¦çš„æœ€åä¸€å¤©, å¦‚æœå·²ç»æ˜¯å­£åº¦æœ«åˆ™è¿”å›åŸå€¼
# def get_quarter_end_date(date_str):
#     date_str = str(date_str)
#     date_check = date_str[-4:]
#     if date_check == '1231' or date_check == '0331' or date_check == '0630' or date_check =='0930':
#         return date_str
#     date = datetime.datetime.strptime(date_str, '%Y%m%d').date()
#     quarter_month = ((date.month-1)//3) * 3 + 1
#     # æ„å»ºå­£åº¦æœ«æ—¥æœŸ
#     quarter_end_date = datetime.date(date.year, quarter_month, 1) + datetime.timedelta(days=-1)
#     # å°†æ—¥æœŸæ ¼å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
#     quarter_end_date_str = quarter_end_date.strftime('%Y%m%d')
#     return quarter_end_date_str


# def gen_group_train_data(groups, mean_data, std_data, daily_data):
#     current_dir = os.path.dirname(os.path.abspath(__file__))
#     train_folder = os.path.join(current_dir, 'train2')
#     for i in tqdm(range(len(groups))):
#         group = groups[i][1]
#         # å»é™¤æœ€å7ä¸ªå­£åº¦çš„æ•°æ®
#         group = group.iloc[:-7]
#         group = group.fillna(0).reset_index(drop=True)
#         group_data_length = len(group)
#         # ç›®å‰å¿…é¡»ä¸Šå¸‚31ä¸ªå­£åº¦åæ‰å¯ä»¥é¢„æµ‹
#         if group_data_length < 31:
#             continue
#         symbol = groups[i][0]
#         group_daily = daily_data[(daily_data['symbol'] == symbol)].reset_index(drop=True)

#         for j in range(31, group_data_length-1):
#             group['endDate'] = group['endDate'].astype('int32')
#             endDate = group['endDate'].iloc[j]
#             data_basename = symbol + '_' + str(endDate) + '.h5'
#             data_name = os.path.join(train_folder, data_basename)

#             # æˆªå–dailyæ•°æ®
#             past_daily = group_daily[(group_daily['date'] <= int(endDate))]
#             past_daily = past_daily.iloc[-127*3:]
#             past_daily = past_daily.reset_index(drop=True)
#             if len(past_daily) != 127*3:
#                 continue
#             max_past = past_daily['close'].max()
#             median_past = past_daily['close'].median()
#             min_past = past_daily['close'].min()
#             math_past = math.log(max_past) + math.log(median_past) + math.log(min_past)
#             ##è·å–oneçš„label
#             fore_daily = group_daily[(group_daily['date'] > int(endDate))]
#             fore_daily = fore_daily.iloc[:127]
#             fore_daily = fore_daily.reset_index(drop=True)
#             if len(fore_daily) != 127:
#                 continue
#             max_fore = fore_daily['close'].max()
#             median_fore = fore_daily['close'].median()
#             min_fore = fore_daily['close'].min()
#             one = math.log(max_fore) + math.log(median_fore) + math.log(min_fore) - math_past
#             if one > 127 or one < -127:
#                 continue
#             ##è·å–threeçš„label
#             fore_daily = group_daily[(group_daily['date'] > int(endDate))]
#             fore_daily = fore_daily.iloc[:127*3]
#             fore_daily = fore_daily.reset_index(drop=True)
#             if len(fore_daily) != 127*3:
#                 continue
#             max_fore = fore_daily['close'].max()
#             median_fore = fore_daily['close'].median()
#             min_fore = fore_daily['close'].min()
#             three = math.log(max_fore) + math.log(median_fore) + math.log(min_fore) - math_past
#             if three > 127 or three < -127:
#                 continue
#             ##è·å–sevençš„label
#             fore_daily = group_daily[(group_daily['date'] > int(endDate))]
#             fore_daily = fore_daily.iloc[:127*7]
#             fore_daily = fore_daily.reset_index(drop=True)
#             if len(fore_daily) != 127*7:
#                 continue
#             max_fore = fore_daily['close'].max()
#             median_fore = fore_daily['close'].median()
#             min_fore = fore_daily['close'].min()
#             seven = math.log(max_fore) + math.log(median_fore) + math.log(min_fore) - math_past
#             if seven > 127 or seven < -127:
#                 continue
#             # æˆªå–dailyæ•°æ®
#             # ä¸‹é¢è¿›è¡Œå½’ä¸€åŒ–
#             # data = group[(group['endDate'] <= int(endDate))].reset_index(drop=True)
#             # data = data.iloc[-17:].reset_index(drop=True)
#             # print(endDate)
#             # print(data)
#             data = group.iloc[j - 30:j + 1].reset_index(drop=True)
#             if len(data) != 31:
#                 continue
#             data.drop(columns=['symbol'], inplace=True)
#             data.drop(columns=['endDate'], inplace=True)
#             col_names = data.columns.values
#             mean_col_names = mean_data.columns.values
#             for k in range(len(data.columns)):
#                 col_name = col_names[k]
#                 if col_name not in mean_col_names:
#                     continue
#                 mean_value = mean_data.loc[mean_data['endDate'] == int(endDate), col_name].item()
#                 std_value = std_data.loc[std_data['endDate'] == int(endDate), col_name].item()
#                 if std_value != 0:
#                     data[col_name] = data[col_name] - mean_value
#                     data[col_name] = data[col_name] / std_value
#                 else:
#                     data[col_name] = 0

#             data = data.assign(one=one)
#             data = data.assign(three=three)
#             data = data.assign(seven=seven)
#             data = data.fillna(0)
#             data.replace([np.inf, -np.inf], 0, inplace=True)
#             for col in data.select_dtypes(include=['int64']).columns:
#                 data[col] = data[col].astype('float32')
#             for col in data.select_dtypes(include=['float64']).columns:
#                 data[col] = data[col].astype('float32')
#             for col in data.select_dtypes(include=['object']).columns:
#                 data[col] = data[col].astype('float32')
#             data.to_hdf(data_name, key='data', mode='w')

# def gen_exchange_three_train_data_pre(exchange):
#     upper_exchange = exchange[0].upper() + exchange[1:]
#     # åŠ è½½æ•°æ®
#     current_dir = os.path.dirname(os.path.abspath(__file__))
#     daily_path = os.path.join(current_dir, '../data/'+ upper_exchange + '/daily_' + exchange + '.csv')
#     daily_data = pd.read_csv(daily_path, encoding="utf-8")
#     income_path = os.path.join(current_dir, '../data/'+ upper_exchange + '/income_' + exchange + '.csv')
#     income_data = pd.read_csv(income_path, encoding="utf-8")
#     balance_path = os.path.join(current_dir, '../data/'+ upper_exchange + '/balance_' + exchange + '.csv')
#     balance_data = pd.read_csv(balance_path, encoding="utf-8")
#     cashflow_path = os.path.join(current_dir, '../data/'+ upper_exchange + '/cashflow_' + exchange + '.csv')
#     cashflow_data = pd.read_csv(cashflow_path, encoding="utf-8")
#     mean_path = os.path.join(current_dir, '../data/'+ upper_exchange + '/mean_' + exchange + '.csv')
#     mean_data = pd.read_csv(mean_path, encoding="utf-8")
#     std_path = os.path.join(current_dir, '../data/'+ upper_exchange + '/std_' + exchange + '.csv')
#     std_data = pd.read_csv(std_path, encoding="utf-8")

#     # åˆå¹¶è´¢åŠ¡ç›¸å…³æ•°æ®
#     financial_data = pd.merge(income_data, balance_data, on=['symbol', 'endDate'], how='outer')
#     financial_data = pd.merge(financial_data, cashflow_data, on=['symbol', 'endDate'], how='outer')
#     financial_data = financial_data.dropna().reset_index(drop=True)
#     financial_data['endDate'] = financial_data['endDate'].apply(get_quarter_end_date)
#     financial_data.drop_duplicates(subset=['symbol', 'endDate'], keep='first', inplace=True)
#     financial_data = financial_data.reset_index(drop=True)

#     #åˆ é™¤financial_dataä¸­endDateå°äºmean std dataä¸­æœ€æ—©çš„endDateçš„æ‰€æœ‰è¡Œ
#     financial_data = financial_data[financial_data['endDate'].astype(float) >= mean_data['endDate'].iloc[0].astype(float)]
#     financial_data = financial_data.reset_index(drop=True)
#     financial_data = financial_data[financial_data['endDate'].astype(float) >= std_data['endDate'].iloc[0].astype(float)]
#     financial_data = financial_data.reset_index(drop=True)
#     print(financial_data)

#     groups = list(financial_data.groupby('symbol'))
#     random.shuffle(groups)
#     group_count = len(groups)
#     split_count = int(0.333*group_count)
#     groups_0 = groups[:split_count]
#     groups_1 = groups[split_count:2*split_count]
#     groups_2 = groups[2*split_count:]

#     # åˆ›å»ºçº¿ç¨‹å¹¶å¯åŠ¨å®ƒä»¬
#     thread0 = threading.Thread(target=gen_group_train_data, args=(groups_0, mean_data, std_data, daily_data))
#     thread1 = threading.Thread(target=gen_group_train_data, args=(groups_1, mean_data, std_data, daily_data))
#     thread2 = threading.Thread(target=gen_group_train_data, args=(groups_2, mean_data, std_data, daily_data))

#     thread0.start()
#     thread1.start()
#     thread2.start()

#     thread0.join()
#     thread1.join()
#     thread2.join()


def process_group(group_tuple, mean_data, std_data, daily_data, train_folder):
    """
    å¤„ç†å•ä¸ªè‚¡ç¥¨æ•°æ®ç»„å¹¶ç”Ÿæˆè®­ç»ƒæ–‡ä»¶ã€‚
    è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å‡½æ•°ï¼Œé€‚åˆåœ¨å¤šè¿›ç¨‹ä¸­è¿è¡Œã€‚
    """
    symbol = group_tuple[0]
    group = group_tuple[1]

    # 1. æ•°æ®æ¸…æ´—å’Œå‡†å¤‡
    # å»é™¤æœ€å7ä¸ªå­£åº¦çš„æ•°æ®
    group = group.iloc[:-7]
    group = group.fillna(0).reset_index(drop=True)
    group_data_length = len(group)

    if group_data_length < 31:
        return 0 # è¿”å›æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°ï¼Œè¿™é‡Œæ˜¯ 0

    # ä¼˜åŒ–ï¼šæå‰ç­›é€‰ daily_data
    group_daily = daily_data[daily_data['symbol'] == symbol].reset_index(drop=True)
    if group_daily.empty:
        return 0
    
    # æå‰è½¬æ¢ç±»å‹
    group['endDate'] = group['endDate'].astype('int32')

    files_created = 0

    # å¾ªç¯ç”Ÿæˆè®­ç»ƒæ ·æœ¬
    for j in range(30, group_data_length-1):
        endDate = group['endDate'].iloc[j] # è¿™é‡Œçš„ endDate åº”è¯¥æ˜¯å½“å‰æ ·æœ¬çš„æœ€æ–°è´¢åŠ¡æŠ¥å‘ŠæœŸ

        # é¢„æµ‹æœŸ daily æ•°æ®
        one_fore_daily = group_daily[group_daily['date'] > int(endDate)].reset_index(drop=True)
        one_fore_daily = one_fore_daily.iloc[:127]
        
        if len(one_fore_daily) != 127:
            continue
        # æ£€æŸ¥é¢„æµ‹æœŸä»·æ ¼æ˜¯å¦æœ‰éæ­£æ•°ï¼ˆ0æˆ–è´Ÿæ•°ï¼‰
        if (one_fore_daily['close'] <= 0).any():
            # å‘ç°éæ­£æ•°ä»·æ ¼ï¼Œè·³è¿‡æ­¤æ ·æœ¬
            continue
        # ä¼˜åŒ–ï¼šä½¿ç”¨ .agg(['min', 'median']) ç®€åŒ–æ“ä½œ
        one_fore_stats = one_fore_daily['close'].agg(['min', 'median'])
        # one_fore_stats = one_fore_daily['close'].agg(['median'])
        one_fore_price = sum(math.log(x) for x in one_fore_stats)

        three_fore_daily = group_daily[group_daily['date'] > int(endDate)].reset_index(drop=True)
        three_fore_daily = three_fore_daily.iloc[:127*3] 
        
        if len(three_fore_daily) != 127*3:
            continue
        # æ£€æŸ¥é¢„æµ‹æœŸä»·æ ¼æ˜¯å¦æœ‰éæ­£æ•°ï¼ˆ0æˆ–è´Ÿæ•°ï¼‰
        if (three_fore_daily['close'] <= 0).any():
            # å‘ç°éæ­£æ•°ä»·æ ¼ï¼Œè·³è¿‡æ­¤æ ·æœ¬
            continue
        # ä¼˜åŒ–ï¼šä½¿ç”¨ .agg(['min', 'median']) ç®€åŒ–æ“ä½œ
        three_fore_stats = three_fore_daily['close'].agg(['min', 'median'])
        # three_fore_stats = three_fore_daily['close'].agg(['median'])
        three_fore_price = sum(math.log(x) for x in three_fore_stats)

        seven_fore_daily = group_daily[group_daily['date'] > int(endDate)].reset_index(drop=True)
        seven_fore_daily = seven_fore_daily.iloc[:127*7] 
        
        if len(seven_fore_daily) != 127*7:
            continue
        # æ£€æŸ¥é¢„æµ‹æœŸä»·æ ¼æ˜¯å¦æœ‰éæ­£æ•°ï¼ˆ0æˆ–è´Ÿæ•°ï¼‰
        if (seven_fore_daily['close'] <= 0).any():
            # å‘ç°éæ­£æ•°ä»·æ ¼ï¼Œè·³è¿‡æ­¤æ ·æœ¬
            continue
        # ä¼˜åŒ–ï¼šä½¿ç”¨ .agg(['min', 'median']) ç®€åŒ–æ“ä½œ
        seven_fore_stats = seven_fore_daily['close'].agg(['min', 'median'])
        # seven_fore_stats = seven_fore_daily['close'].agg(['median'])
        seven_fore_price = sum(math.log(x) for x in seven_fore_stats)

        # å†å²æœŸ daily æ•°æ® (åœ¨ endDate_split ä¹‹å‰ï¼Œå–æœ€è¿‘ 127*3 å¤©)
        past_daily = group_daily[group_daily['date'] <= int(endDate)].reset_index(drop=True)
        past_daily = past_daily.iloc[-127*3:]
        
        if len(past_daily) != 127*3:
            continue
        # æ£€æŸ¥å†å²æœŸä»·æ ¼æ˜¯å¦æœ‰éæ­£æ•°ï¼ˆ0æˆ–è´Ÿæ•°ï¼‰
        if (past_daily['close'] <= 0).any():
            # å‘ç°éæ­£æ•°ä»·æ ¼ï¼Œè·³è¿‡æ­¤æ ·æœ¬
            continue
        # ä¼˜åŒ–ï¼šä½¿ç”¨ .agg([ 'median', 'max']) ç®€åŒ–æ“ä½œ
        past_stats = past_daily['close'].agg(['median', 'max'])
        # past_stats = past_daily['close'].agg(['median'])
        past_price = sum(math.log(x) for x in past_stats)
        
        one = one_fore_price - past_price
        three = three_fore_price - past_price
        seven = seven_fore_price - past_price
        
        if abs(one) > 127 or abs(three) > 127 or abs(seven) > 127: # å¼‚å¸¸å€¼è¿‡æ»¤
            continue
            
        # 2. è®­ç»ƒè¾“å…¥ (è´¢åŠ¡æ¯”ç‡) å‡†å¤‡
        
        # æˆªå– 3 ä¸ªå­£åº¦çš„è´¢åŠ¡æ•°æ® (j-2, j-1, j)
        data = group.iloc[j - 30:j + 1].copy().reset_index(drop=True) # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
        if len(data) != 31:
            continue
        
        data.drop(columns=['symbol', 'endDate'], inplace=True)
        col_names = data.columns.values
        mean_col_names = mean_data.columns.values
        for k in range(len(data.columns)):
            col_name = col_names[k]
            if col_name not in mean_col_names:
                continue
            mean_value = mean_data.loc[mean_data['endDate'] == int(endDate), col_name].item()
            std_value = std_data.loc[std_data['endDate'] == int(endDate), col_name].item()
            if std_value != 0:
                data[col_name] = data[col_name] - mean_value
                data[col_name] = data[col_name] / std_value
            else:
                data[col_name] = 0
        data = data.assign(one=one)
        data = data.assign(three=three)
        data = data.assign(seven=seven)
        data = data.fillna(0)
        data.replace([np.inf, -np.inf], 0, inplace=True)
        data[(data > 8191.0)] = 8191.0
        data[(data < -8191.0)] = -8191.0
        for col in data.columns:
            if data[col].dtype in ['int64', 'float64', 'object']:
                data[col] = data[col].astype('float32')

        # ä¿å­˜ä¸º HDF5
        data_basename = f"{symbol}_{endDate}.h5"
        data_name = os.path.join(train_folder, data_basename)
        
        # ä½¿ç”¨ 'a' æ¨¡å¼å†™å…¥å•ä¸ªæ–‡ä»¶æ›´å®‰å…¨ï¼Œè™½ç„¶ 'w' ä¹Ÿå¯ä»¥
        data.to_hdf(data_name, key='data', mode='w') 
        files_created += 1
        
    return files_created


def gen_exchange_three_train_data(exchange):
    """
    åŠ è½½æ•°æ®ï¼Œåˆ†å‰²ä»»åŠ¡å¹¶ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ã€‚
    """
    upper_exchange = exchange[0].upper() + exchange[1:]
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # 1. åŠ è½½æ•°æ®
    daily_path = os.path.join(current_dir, f'../data/{upper_exchange}/daily_{exchange}.csv')
    income_path = os.path.join(current_dir, f'../data/{upper_exchange}/income_{exchange}.csv')
    balance_path = os.path.join(current_dir, f'../data/{upper_exchange}/balance_{exchange}.csv')
    cashflow_path = os.path.join(current_dir, f'../data/{upper_exchange}/cashflow_{exchange}.csv')
    mean_path = os.path.join(current_dir, f'../data/{upper_exchange}/mean_{exchange}.csv')
    std_path = os.path.join(current_dir, f'../data/{upper_exchange}/std_{exchange}.csv')
    indicator_path = os.path.join(current_dir, f'../data/{upper_exchange}/indicator_{exchange}.csv')
    features_path = os.path.join(current_dir, f'../three/features_importance.csv')

    print(f"Loading {exchange} data...")
    try:
        daily_data = pd.read_csv(daily_path, encoding="utf-8")
        income_data = pd.read_csv(income_path, encoding="utf-8")
        balance_data = pd.read_csv(balance_path, encoding="utf-8")
        cashflow_data = pd.read_csv(cashflow_path, encoding="utf-8")
        mean_data = pd.read_csv(mean_path, encoding="utf-8")
        std_data = pd.read_csv(std_path, encoding="utf-8")
        indicator_data = pd.read_csv(indicator_path, encoding="utf-8")
        features = pd.read_csv(features_path, encoding="utf-8")
        # åˆå¹¶è´¢åŠ¡ç›¸å…³æ•°æ®
        financial_data = pd.merge(income_data, balance_data, on=['symbol', 'endDate'], how='outer')
        financial_data = pd.merge(financial_data, cashflow_data, on=['symbol', 'endDate'], how='outer')
        financial_data = financial_data.dropna(subset=['symbol', 'endDate'])
        financial_data = financial_data.fillna(0).reset_index(drop=True)
        financial_data.drop_duplicates(subset=['symbol', 'endDate'], keep='first', inplace=True)
        financial_data = financial_data.reset_index(drop=True)
        # æˆªå–æŒ‡å®šç‰¹å¾çš„éƒ¨åˆ†
        features_list = features['feature'].to_list()
        features_data = pd.DataFrame()
        features_data['symbol'] = financial_data['symbol']
        features_data['endDate'] = financial_data['endDate'].astype('int64')
        for feature in features_list:
            features_data[feature] = financial_data[feature]
        features_data = pd.merge(indicator_data, features_data, on=['symbol', 'endDate'], how='outer')
        features_data = features_data.dropna(subset=['symbol', 'endDate'])
        features_data = features_data.fillna(0).reset_index(drop=True)
        features_data.drop_duplicates(subset=['symbol', 'endDate'], keep='first', inplace=True)
        features_data = features_data.reset_index(drop=True)

        #åˆ é™¤features_dataä¸­endDateå°äºmean std dataä¸­æœ€æ—©çš„endDateçš„æ‰€æœ‰è¡Œ
        features_data = features_data[features_data['endDate'].astype(float) >= mean_data['endDate'].iloc[0].astype(float)]
        features_data = features_data.reset_index(drop=True)
        features_data = features_data[features_data['endDate'].astype(float) >= std_data['endDate'].iloc[0].astype(float)]
        features_data = features_data.reset_index(drop=True)
    except FileNotFoundError:
        print(f"Error: Data files not found for {exchange}.")
        return 0
        
    # 2. åˆ†ç»„å¹¶æ‰“ä¹±é¡ºåº
    groups = list(features_data.groupby('symbol'))
    random.shuffle(groups)

    # 3. è®¾ç½®å¤šè¿›ç¨‹å‚æ•°
    train_folder = os.path.join(current_dir, 'train')
    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨ CPU æ ¸å¿ƒï¼Œæˆ–æ ¹æ®éœ€è¦è®¾ç½®ä¸€ä¸ªå›ºå®šå€¼
    num_processes = cpu_count() 
    print(f"Starting {num_processes} processes for {len(groups)} groups.")
    
    # å‡†å¤‡ Pool.starmap éœ€è¦çš„å‚æ•°åˆ—è¡¨
    # (group_tuple, daily_data, train_folder)
    task_args = [(group_tuple, mean_data, std_data, daily_data, train_folder) for group_tuple in groups]
    
    # 4. è¿è¡Œå¤šè¿›ç¨‹æ± 
    total_files_created = 0
    try:
        # ä½¿ç”¨ Pool.starmap å¹¶è¡Œå¤„ç†æ‰€æœ‰ groups
        with Pool(processes=num_processes) as pool:
            # è¿›ç¨‹æ± ä¼šè¿”å›ä¸€ä¸ªç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœæ˜¯ process_group çš„è¿”å›å€¼ (files_created)
            results = list(tqdm(pool.starmap(process_group, task_args), total=len(groups), desc=f"Processing {exchange} groups"))
        
        total_files_created = sum(results)
        print(f"Finished {exchange}. Total files created: {total_files_created}")
        
    except Exception as e:
        print(f"An error occurred during multiprocessing for {exchange}: {e}")
        return 0

    return total_files_created


def gen_three_train_data():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    train_folder = os.path.join(current_dir, 'train')
    
    # æ¸…ç†å’Œåˆ›å»ºè®­ç»ƒç›®å½•
    print("Clearing and creating 'train' folder...")
    shutil.rmtree(train_folder, ignore_errors=True)
    os.makedirs(train_folder, exist_ok=True)
    
    # è·å–äº¤æ˜“æ‰€åˆ—è¡¨
    csv_path = os.path.join(current_dir, '..', 'train_exchanges.csv')
    csv_path = os.path.abspath(csv_path)

    try:
        exchanges_data = pd.read_csv(csv_path, encoding="utf-8")
        exchange_list = exchanges_data['exchange'].tolist()
        # exchange_list = ['KOE']
    except FileNotFoundError:
        print(f"Error: train_exchanges.csv not found at {csv_path}")
        return

    failed_list = []
    total_files = 0
    
    for exchange in exchange_list:
        try:
            print(f"\n--- Starting processing for {exchange} ---")
            files_created = gen_exchange_three_train_data(exchange)
            if files_created == 0:
                 failed_list.append(exchange)
            total_files += files_created
        except Exception as e:
            print(f"FATAL error for exchange {exchange}: {e}")
            failed_list.append(exchange)

    print("\n--- Summary ---")
    print(f"Total HDF5 files created: {total_files}")
    if failed_list:
        print(f"Failed to process exchanges: {failed_list} âš ï¸")
    else:
        print("All exchanges processed successfully! ğŸ‰")


if __name__ == '__main__':
    gen_three_train_data()
    # gen_exchange_three_train_data_pre('KOE')