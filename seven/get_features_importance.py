import torch
import torch.nn as nn
from captum.attr import IntegratedGradients
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from three_model import THREE
import os
import random
from tqdm import tqdm
from collections import Counter

def get_data_input(data):
    input_data = data.iloc[:, :-3]
    return input_data

def get_features_importance(model_name):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“¦ Using device: {device}")

    # ====== æ¨¡å‹åŠ è½½ ======
    model = THREE([118, 31], [1, 1]).to(device)
    # model.load(torch.load(model_name, map_location=device))
    model = torch.load(model_name)
    model.eval()

    # ====== æ•°æ®å‡†å¤‡ ======
    train_folder = 'train'
    h5_files_list = [os.path.join(train_folder, f) for f in os.listdir(train_folder) if f.endswith('.h5')]
    feature_list = []
    for i in tqdm(range(127)):
        random.shuffle(h5_files_list)
        h5_files = h5_files_list[:31]

        # å–ç¬¬ä¸€ä¸ªæ–‡ä»¶åˆ—å
        data = pd.read_hdf(h5_files[0])
        feature_names = data.columns.tolist()[:118]

        # ç»„è£… batch
        X_test = []
        for h5_file in h5_files:
            data = pd.read_hdf(h5_file)
            input_data = get_data_input(data)
            input_tensor = torch.tensor(input_data.values, dtype=torch.float32, device=device)
            X_test.append(input_tensor)

        X = torch.stack(X_test, dim=0).detach().clone().requires_grad_(True)  # [127, 7, 10]
        baseline = torch.zeros_like(X)

        # ====== Integrated Gradients ======
        def model_for_captum(x):
            _, one, _, three, _, seven = model(x)
            growth_death = one + three + seven
            return growth_death  # åªè¿”å›ç¬¬ä¸€ä¸ªè¾“å‡º
        ig = IntegratedGradients(model_for_captum)

        attributions, _ = ig.attribute(inputs=X, baselines=baseline, return_convergence_delta=True)

        # ====== æ±‡æ€»ç‰¹å¾é‡è¦æ€§ ======
        feature_importance = attributions.abs().mean(dim=(0, 1)).detach().cpu().numpy()
        # è½¬æˆ DataFrame
        features_ranking = pd.DataFrame({
            "feature": feature_names,
            "importance": feature_importance
        })
        # æŒ‰é‡è¦æ€§é™åºæ’åº
        features_ranking = features_ranking.sort_values(by="importance", ascending=False).reset_index(drop=True)
        features_ranking = features_ranking.iloc[:86]
        # print(features_ranking)
        features = features_ranking['feature'].to_list()
        feature_list = feature_list + features
    counter = Counter(feature_list)
    features_count = pd.DataFrame(list(counter.items()), columns=['feature', 'count'])
    features_count = features_count.sort_values('count', ascending=False)
    features_count = features_count.reset_index(drop=True)
    features_count = features_count.iloc[:86]
    print(features_count)
    features_count_pre = pd.read_csv('features_importance.csv')
    features_list_pre = features_count_pre['feature'].to_list()
    features_list = features_count['feature'].to_list()
    add_features = list(set(features_list) - set(features_list_pre))
    print('add_features', add_features)
    delete_features = list(set(features_list_pre) - set(features_list))
    print('delete_features', delete_features)
    features_count.to_csv('features_importance.csv', index=False)

if __name__ == '__main__':
    model_name = 'three.pt'
    get_features_importance(model_name)