import os
import pandas as pd
import random
import numpy as np
import math
import threading
from tqdm import tqdm
import shutil
import datetime
from multiprocessing import Pool, cpu_count

pd.set_option('future.no_silent_downcasting', True)

def process_group(group_tuple, mean_data, std_data, train_folder):
    """
    å¤„ç†å•ä¸ªè‚¡ç¥¨æ•°æ®ç»„å¹¶ç”Ÿæˆè®­ç»ƒæ–‡ä»¶ã€‚
    è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å‡½æ•°ï¼Œé€‚åˆåœ¨å¤šè¿›ç¨‹ä¸­è¿è¡Œã€‚
    """
    symbol = group_tuple[0]
    group = group_tuple[1]

    # 1. æ•°æ®æ¸…æ´—å’Œå‡†å¤‡
    group = group.fillna(0).reset_index(drop=True)
    group_data_length = len(group)

    if group_data_length < 62:
        return 0 # è¿”å›æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°ï¼Œè¿™é‡Œæ˜¯ 0
    
    # æå‰è½¬æ¢ç±»å‹
    group['endDate'] = group['endDate'].astype('int32')

    files_created = 0

    # å¾ªç¯ç”Ÿæˆè®­ç»ƒæ ·æœ¬
    for j in range(30, group_data_length-1):
        endDate = group['endDate'].iloc[j] # è¿™é‡Œçš„ endDate åº”è¯¥æ˜¯å½“å‰æ ·æœ¬çš„æœ€æ–°è´¢åŠ¡æŠ¥å‘ŠæœŸ

        data = group.iloc[j - 30:j + 1].copy().reset_index(drop=True) # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
        data_fore = group.iloc[j + 1:j + 32].copy().reset_index(drop=True) # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
        if len(data) != 31:
            continue
        if len(data_fore) != 31:
            continue
        totalStockholdersEquity = data['totalStockholdersEquity'].iloc[-1]
        three_dcf_fore = data_fore['freeCashFlow'].iloc[:3].sum()
        seven_dcf_fore = data_fore['freeCashFlow'].iloc[:7].sum()
        thirty_one_dcf_fore = data_fore['freeCashFlow'].iloc[:31].sum()
        # æ£€æŸ¥æ˜¯ä¸æ˜¯æœ‰ç‚¹å‡€èµ„äº§
        if totalStockholdersEquity < 524287.0:
            continue
        
        three = three_dcf_fore/totalStockholdersEquity
        seven = seven_dcf_fore/totalStockholdersEquity
        thirty_one = thirty_one_dcf_fore/totalStockholdersEquity
        
        if abs(three) > 127 or abs(seven) > 127 or abs(thirty_one) > 127: # å¼‚å¸¸å€¼è¿‡æ»¤
            continue
            
        # 2. è®­ç»ƒè¾“å…¥ (è´¢åŠ¡æ¯”ç‡) å‡†å¤‡
        
        data.drop(columns=['symbol', 'endDate'], inplace=True)
        col_names = data.columns.values
        mean_col_names = mean_data.columns.values
        for k in range(len(data.columns)):
            col_name = col_names[k]
            if col_name not in mean_col_names:
                continue
            mean_value = mean_data.loc[mean_data['endDate'] == int(endDate), col_name].item()
            std_value = std_data.loc[std_data['endDate'] == int(endDate), col_name].item()
            if std_value != 0:
                data[col_name] = data[col_name] - mean_value
                data[col_name] = data[col_name] / std_value
            else:
                data[col_name] = 0
        data = data.assign(three=three)
        data = data.assign(seven=seven)
        data = data.assign(thirty_one=thirty_one)
        data = data.fillna(0)
        data.replace([np.inf, -np.inf], 0, inplace=True)
        data[(data > 8191.0)] = 8191.0
        data[(data < -8191.0)] = -8191.0
        for col in data.columns:
            if data[col].dtype in ['int64', 'float64', 'object']:
                data[col] = data[col].astype('float32')

        # ä¿å­˜ä¸º HDF5
        data_basename = f"{symbol}_{endDate}.h5"
        data_name = os.path.join(train_folder, data_basename)
        
        # ä½¿ç”¨ 'a' æ¨¡å¼å†™å…¥å•ä¸ªæ–‡ä»¶æ›´å®‰å…¨ï¼Œè™½ç„¶ 'w' ä¹Ÿå¯ä»¥
        data.to_hdf(data_name, key='data', mode='w') 
        files_created += 1
        
    return files_created


def gen_exchange_seven_train_data(exchange):
    """
    åŠ è½½æ•°æ®ï¼Œåˆ†å‰²ä»»åŠ¡å¹¶ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ã€‚
    """
    upper_exchange = exchange[0].upper() + exchange[1:]
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # 1. åŠ è½½æ•°æ®
    income_path = os.path.join(current_dir, f'../data/{upper_exchange}/income_{exchange}.csv')
    balance_path = os.path.join(current_dir, f'../data/{upper_exchange}/balance_{exchange}.csv')
    cashflow_path = os.path.join(current_dir, f'../data/{upper_exchange}/cashflow_{exchange}.csv')
    mean_path = os.path.join(current_dir, f'../data/{upper_exchange}/mean_{exchange}.csv')
    std_path = os.path.join(current_dir, f'../data/{upper_exchange}/std_{exchange}.csv')
    indicator_path = os.path.join(current_dir, f'../data/{upper_exchange}/indicator_{exchange}.csv')
    features_path = os.path.join(current_dir, f'../three/features_importance.csv')

    print(f"Loading {exchange} data...")
    try:
        income_data = pd.read_csv(income_path, encoding="utf-8")
        balance_data = pd.read_csv(balance_path, encoding="utf-8")
        cashflow_data = pd.read_csv(cashflow_path, encoding="utf-8")
        mean_data = pd.read_csv(mean_path, encoding="utf-8")
        std_data = pd.read_csv(std_path, encoding="utf-8")
        indicator_data = pd.read_csv(indicator_path, encoding="utf-8")
        features = pd.read_csv(features_path, encoding="utf-8")
        # åˆå¹¶è´¢åŠ¡ç›¸å…³æ•°æ®
        financial_data = pd.merge(income_data, balance_data, on=['symbol', 'endDate'], how='outer')
        financial_data = pd.merge(financial_data, cashflow_data, on=['symbol', 'endDate'], how='outer')
        financial_data = financial_data.dropna(subset=['symbol', 'endDate'])
        financial_data = financial_data.fillna(0).reset_index(drop=True)
        financial_data.drop_duplicates(subset=['symbol', 'endDate'], keep='first', inplace=True)
        financial_data = financial_data.reset_index(drop=True)
        # æˆªå–æŒ‡å®šç‰¹å¾çš„éƒ¨åˆ†
        features_list = features['feature'].to_list()
        features_data = pd.DataFrame()
        features_data['symbol'] = financial_data['symbol']
        features_data['endDate'] = financial_data['endDate'].astype('int64')
        for feature in features_list:
            features_data[feature] = financial_data[feature]
        features_data = pd.merge(indicator_data, features_data, on=['symbol', 'endDate'], how='outer')
        features_data = features_data.dropna(subset=['symbol', 'endDate'])
        features_data = features_data.fillna(0).reset_index(drop=True)
        features_data.drop_duplicates(subset=['symbol', 'endDate'], keep='first', inplace=True)
        features_data = features_data.reset_index(drop=True)

        #åˆ é™¤features_dataä¸­endDateå°äºmean std dataä¸­æœ€æ—©çš„endDateçš„æ‰€æœ‰è¡Œ
        features_data = features_data[features_data['endDate'].astype(float) >= mean_data['endDate'].iloc[0].astype(float)]
        features_data = features_data.reset_index(drop=True)
        features_data = features_data[features_data['endDate'].astype(float) >= std_data['endDate'].iloc[0].astype(float)]
        features_data = features_data.reset_index(drop=True)
    except FileNotFoundError:
        print(f"Error: Data files not found for {exchange}.")
        return 0
        
    # 2. åˆ†ç»„å¹¶æ‰“ä¹±é¡ºåº
    groups = list(features_data.groupby('symbol'))
    random.shuffle(groups)

    # 3. è®¾ç½®å¤šè¿›ç¨‹å‚æ•°
    train_folder = os.path.join(current_dir, 'train')
    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨ CPU æ ¸å¿ƒï¼Œæˆ–æ ¹æ®éœ€è¦è®¾ç½®ä¸€ä¸ªå›ºå®šå€¼
    num_processes = cpu_count() 
    print(f"Starting {num_processes} processes for {len(groups)} groups.")
    
    # å‡†å¤‡ Pool.starmap éœ€è¦çš„å‚æ•°åˆ—è¡¨
    # (group_tuple, daily_data, train_folder)
    task_args = [(group_tuple, mean_data, std_data, train_folder) for group_tuple in groups]
    
    # 4. è¿è¡Œå¤šè¿›ç¨‹æ± 
    total_files_created = 0
    try:
        # ä½¿ç”¨ Pool.starmap å¹¶è¡Œå¤„ç†æ‰€æœ‰ groups
        with Pool(processes=num_processes) as pool:
            # è¿›ç¨‹æ± ä¼šè¿”å›ä¸€ä¸ªç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœæ˜¯ process_group çš„è¿”å›å€¼ (files_created)
            results = list(tqdm(pool.starmap(process_group, task_args), total=len(groups), desc=f"Processing {exchange} groups"))
        
        total_files_created = sum(results)
        print(f"Finished {exchange}. Total files created: {total_files_created}")
        
    except Exception as e:
        print(f"An error occurred during multiprocessing for {exchange}: {e}")
        return 0

    return total_files_created


def gen_seven_train_data():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    train_folder = os.path.join(current_dir, 'train')
    
    # æ¸…ç†å’Œåˆ›å»ºè®­ç»ƒç›®å½•
    print("Clearing and creating 'train' folder...")
    shutil.rmtree(train_folder, ignore_errors=True)
    os.makedirs(train_folder, exist_ok=True)
    
    # è·å–äº¤æ˜“æ‰€åˆ—è¡¨
    csv_path = os.path.join(current_dir, '..', 'train_exchanges.csv')
    csv_path = os.path.abspath(csv_path)

    try:
        exchanges_data = pd.read_csv(csv_path, encoding="utf-8")
        exchange_list = exchanges_data['exchange'].tolist()
        # exchange_list = ['KOE']
    except FileNotFoundError:
        print(f"Error: train_exchanges.csv not found at {csv_path}")
        return

    failed_list = []
    total_files = 0
    
    for exchange in exchange_list:
        try:
            print(f"\n--- Starting processing for {exchange} ---")
            files_created = gen_exchange_seven_train_data(exchange)
            if files_created == 0:
                 failed_list.append(exchange)
            total_files += files_created
        except Exception as e:
            print(f"FATAL error for exchange {exchange}: {e}")
            failed_list.append(exchange)

    print("\n--- Summary ---")
    print(f"Total HDF5 files created: {total_files}")
    if failed_list:
        print(f"Failed to process exchanges: {failed_list} âš ï¸")
    else:
        print("All exchanges processed successfully! ğŸ‰")


if __name__ == '__main__':
    gen_seven_train_data()
    # gen_exchange_three_train_data_pre('KOE')